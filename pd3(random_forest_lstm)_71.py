# -*- coding: utf-8 -*-
"""PD3(Random Forest/LSTM)-71.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Aty2vVb-7u_GJ_U7DQ4AhG48fxbiMEgf
"""

import pandas as pd

# Load dataset
file_path = "/content/human_vital_signs_dataset_2024.csv"
df = pd.read_csv(file_path)

# Display basic information
print(df.info())
print(df.head())

"""Data Preprocessing"""

import seaborn as sns
import matplotlib.pyplot as plt

# Plot the correct histogram with actual heart rate values
plt.figure(figsize=(8, 5))
sns.histplot(df["Heart Rate"], bins=50, kde=True, color="blue", alpha=0.6)
plt.title("Heart Rate Distribution")
plt.xlabel("Heart Rate (bpm)")
plt.ylabel("Count")
plt.show()

# Define normal ranges for vital signs
heart_rate_range = (40, 200)  # Normal: 60-100 bpm
respiratory_rate_range = (8, 40)  # Normal: 12-20 bpm
temperature_range = (30, 45)  # Normal: 36-37.5Â°C
oxygen_saturation_range = (70, 100)  # Normal: 95-100%

# Check for invalid data points
invalid_heart_rate = df[(df["Heart Rate"] < heart_rate_range[0]) | (df["Heart Rate"] > heart_rate_range[1])]
invalid_temperature = df[(df["Body Temperature"] < temperature_range[0]) | (df["Body Temperature"] > temperature_range[1])]
invalid_oxygen = df[(df["Oxygen Saturation"] < oxygen_saturation_range[0]) | (df["Oxygen Saturation"] > oxygen_saturation_range[1])]

# Print number of invalid values found
print(f"Invalid Heart Rate values: {len(invalid_heart_rate)}")
print(f"Invalid Body Temperature values: {len(invalid_temperature)}")
print(f"Invalid Oxygen Saturation values: {len(invalid_oxygen)}")

df["Timestamp"] = pd.to_datetime(df["Timestamp"])

from sklearn.preprocessing import MinMaxScaler

# Select numerical columns to normalize
num_cols = ["Heart Rate", "Respiratory Rate", "Body Temperature",
            "Oxygen Saturation", "Systolic Blood Pressure", "Diastolic Blood Pressure"]

scaler = MinMaxScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])

df["Gender"] = df["Gender"].map({"Male": 0, "Female": 1})
df["Risk Category"] = df["Risk Category"].map({"Low Risk": 0, "High Risk": 1})

print(df.head())

"""Exploratory Data Analysis (EDA)"""

import matplotlib.pyplot as plt
import seaborn as sns

# Heart rate distribution
sns.histplot(df["Heart Rate"], bins=50, kde=True)
plt.title("Heart Rate Distribution")
plt.show()

print(df["Heart Rate"].describe())

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

"""This correlation matrix presents the relationships between various health metrics and demographic factors for a set of patients. Here's a breakdown of the interpretation:

1. **Correlation Coefficient Range**:
   - Values close to **1** indicate a strong positive correlation (as one variable increases, the other also tends to increase).
   - Values close to **-1** indicate a strong negative correlation (as one variable increases, the other tends to decrease).
   - Values around **0** suggest little to no correlation.

2. **Key Correlations**:
   - **Derived_Pulse_Pressure** shows a strong positive correlation with **Systolic Blood Pressure (0.83)** and a moderate correlation with **Diastolic Blood Pressure (0.55)**. This suggests that as pulse pressure increases, both systolic and diastolic pressures tend to increase as well.
   - **Derived_MAP (Mean Arterial Pressure)** has a strong correlation with **Systolic Blood Pressure (0.60)** and **Derived_Pulse_Pressure (0.80)**, indicating interrelated cardiovascular measures.
   - **Weight (kg)** and **Height (m)** have a moderate positive correlation (+0.75), which is expected as taller individuals typically weigh more.
   - **Heart Rate** has low correlations with other metrics, indicating that it may not be strongly influenced by the other factors in this dataset.

3. **Negative Correlations**:
   - The correlation between **Risk Category** and **Systolic Blood Pressure (-0.15)** suggests a slight inverse relationship, meaning higher systolic blood pressure may be associated with a lower risk category according to how this dataset defines risk.

4. **Demographic Factors**:
   - There is a strong relationship between **Age** and some derived metrics (like **Derived_BMI** at 0.75), suggesting that age may influence body mass index, which is a common measure in healthcare.

5. **General Observations**:
   - The correlations are largely moderate to strong, indicating that the variables are interconnected, which is useful for understanding patient health risk profiles.
   - This matrix can guide healthcare professionals in identifying potential factors that could be monitored or managed to improve patient outcomes.

In summary, this correlation matrix illustrates important relationships among health metrics, which can be leveraged for predictive modeling and clinical decision-making.

Feature Engineering
"""

df["Hour"] = df["Timestamp"].dt.hour
df["Day"] = df["Timestamp"].dt.day
df["Month"] = df["Timestamp"].dt.month

df["Health Score"] = (
    df["Heart Rate"] * 0.3 + df["Systolic Blood Pressure"] * 0.4 + df["Body Temperature"] * 0.3
)

"""Model Development"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Select features and target
X = df[["Heart Rate", "Body Temperature",
        "Oxygen Saturation", "Systolic Blood Pressure"]]
y = df["Risk Category"]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Predict and evaluate
y_pred = clf.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%")

# Drop weak and redundant features
df = df.drop(columns=["Respiratory Rate", "Timestamp", "Gender", "Age", "Systolic Blood Pressure", "Weight (kg)"])

# Separate features and target
X = df.drop(columns=["Risk Category"])
y = df["Risk Category"]

# Train & Evaluate Random Forest
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Print Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Updated Model Accuracy: {accuracy * 100:.2f}%")

print(df["Risk Category"].value_counts())

from imblearn.over_sampling import SMOTE

smote = SMOTE()
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Initialize Random Forest
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train on resampled data
model.fit(X_train_resampled, y_train_resampled)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate performance
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

print(X_train_resampled.head())  # Inspect first few rows
print(y_train_resampled.value_counts())  # Check balance

X_train_resampled = X_train_resampled.drop(["Patient ID", "Hour", "Day", "Month"], axis=1)

X_train_resampled = X_train_resampled[
    ["Heart Rate", "Oxygen Saturation", "Body Temperature",
     "Diastolic Blood Pressure"]
]

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Split train and test data
X_train, X_test, y_train, y_test = train_test_split(X_train_resampled, y_train_resampled, test_size=0.2, random_state=42)

# Train a new Random Forest model
model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
model.fit(X_train, y_train)

# Evaluate the model
accuracy = model.score(X_test, y_test)
print(f"Updated Model Accuracy: {accuracy:.2%}")

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Updated Model Accuracy: {accuracy:.2%}")

# Detailed classification report (precision, recall, F1-score)
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

# Display confusion matrix as a plot
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=model.classes_)
disp.plot(cmap='Blues')

import pickle

# Save the trained Random Forest model to a file
with open("random_forest_model.pkl", "wb") as file:
    pickle.dump(best_rf, file)

print("Model saved successfully using pickle!")

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(X_train, y_train)

importances = model.feature_importances_
feature_names = X_train.columns

# Sort features by importance
sorted_indices = importances.argsort()[::-1]
for i in sorted_indices:
    print(f"{feature_names[i]}: {importances[i]:.4f}")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Reshape data for LSTM (samples, time steps, features)
X_train_lstm = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_lstm = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Define LSTM model
model = Sequential([
    LSTM(50, activation="relu", return_sequences=True, input_shape=(1, X_train.shape[1])),
    LSTM(50, activation="relu"),
    Dense(1, activation="sigmoid")
])

model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
model.fit(X_train_lstm, y_train, epochs=20, batch_size=32, validation_data=(X_test_lstm, y_test))

# Evaluate model
loss, acc = model.evaluate(X_test_lstm, y_test)
print(f"LSTM Accuracy: {acc * 100:.2f}%")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

# Predict on the test set
y_pred = model.predict(X_test_lstm)

# Convert predictions to binary (0 or 1) for classification
y_pred_classes = (y_pred > 0.5).astype(int)

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_classes)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot(cmap="Blues")
plt.title("LSTM Confusion Matrix")
plt.show()

# Classification report (precision, recall, F1-score)
print("\nClassification Report:")
print(classification_report(y_test, y_pred_classes))